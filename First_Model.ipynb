{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1_DistillBert"
      ],
      "metadata": {
        "id": "7LrfFXKER6ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install pdfplumber transformers torch sentence-transformers\n",
        "\n",
        "# Import libraries\n",
        "from google.colab import drive\n",
        "import pdfplumber\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering, Trainer, TrainingArguments\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 1: Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text: {e}\")\n",
        "    return text\n",
        "\n",
        "# Replace with your PDF path\n",
        "pdf_path = '/content/cse.pdf'  # Update this path\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "print(f\"Extracted text length: {len(pdf_text)} characters\")\n",
        "\n",
        "# Step 2: Preprocess and chunk text\n",
        "def chunk_text(text, max_length=512):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) < max_length:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "text_chunks = chunk_text(pdf_text)\n",
        "print(f\"Number of chunks: {len(text_chunks)}\")\n",
        "\n",
        "# Step 3: Generate synthetic question-answer pairs\n",
        "def generate_synthetic_qa(chunks, num_questions=100):\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    qa_pairs = []\n",
        "    for chunk in tqdm(chunks, desc=\"Generating QA pairs\"):\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', chunk)\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.strip()) > 20:\n",
        "                question = f\"What is mentioned about {sentence[:30].strip()}...?\"\n",
        "                answer = sentence.strip()\n",
        "                qa_pairs.append({\"question\": question, \"answer\": answer, \"context\": chunk})\n",
        "                if len(qa_pairs) >= num_questions:\n",
        "                    break\n",
        "        if len(qa_pairs) >= num_questions:\n",
        "            break\n",
        "    return qa_pairs\n",
        "\n",
        "qa_pairs = generate_synthetic_qa(text_chunks)\n",
        "print(f\"Generated {len(qa_pairs)} QA pairs\")\n",
        "\n",
        "# Step 4: Prepare dataset for fine-tuning\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "class QADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, qa_pairs):\n",
        "        self.qa_pairs = qa_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        qa = self.qa_pairs[idx]\n",
        "        question = qa['question']\n",
        "        context = qa['context']\n",
        "        answer = qa['answer']\n",
        "\n",
        "        # Encode question and context\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            question,\n",
        "            context,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Find start and end positions of answer in context\n",
        "        answer_encoding = self.tokenizer.encode(answer, add_special_tokens=False)\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        answer_ids = self.tokenizer.encode(answer, add_special_tokens=False)\n",
        "\n",
        "        start_positions = -1\n",
        "        end_positions = -1\n",
        "        for i in range(len(input_ids) - len(answer_ids)):\n",
        "            if input_ids[i:i+len(answer_ids)].tolist() == answer_ids:\n",
        "                start_positions = i\n",
        "                end_positions = i + len(answer_ids) - 1\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'token_type_ids': encoding['token_type_ids'].squeeze(),\n",
        "            'start_positions': start_positions,\n",
        "            'end_positions': end_positions\n",
        "        }\n",
        "\n",
        "dataset = QADataset(qa_pairs)\n",
        "\n",
        "# Step 5: Fine-tune the model\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/qa_model',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,  # Enable mixed precision for GPU\n",
        "    logging_dir='/content/drive/MyDrive/logs',\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Step 6: Save the model\n",
        "model.save_pretrained('/content/drive/MyDrive/qa_model')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/qa_model')\n",
        "print(\"Model and tokenizer saved to /content/drive/MyDrive/qa_model\")\n",
        "\n",
        "# Step 7: Function to answer questions using the saved model\n",
        "def answer_question(question, context, model_path='/content/drive/MyDrive/qa_model'):\n",
        "    model = DistilBertForQuestionAnswering.from_pretrained(model_path)\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        question,\n",
        "        context,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    start_idx = torch.argmax(start_scores)\n",
        "    end_idx = torch.argmax(end_scores) + 1\n",
        "\n",
        "    answer_tokens = input_ids[0][start_idx:end_idx]\n",
        "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "question = \"What is the main topic of the document?\"\n",
        "context = text_chunks[0]  # Use first chunk as context or select relevant chunk\n",
        "answer = answer_question(question, context)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVaL7c08PYAi",
        "outputId": "e20f6a73-8981-41a0-fdab-392bf6afbcce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2_roberta-large"
      ],
      "metadata": {
        "id": "dqXweN4NSDCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install pdfplumber transformers torch sentence-transformers\n",
        "\n",
        "# Import libraries\n",
        "from google.colab import drive\n",
        "import pdfplumber\n",
        "import torch\n",
        "from transformers import RobertaTokenizerFast, RobertaForQuestionAnswering, Trainer, TrainingArguments\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 1: Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text: {e}\")\n",
        "    return text\n",
        "\n",
        "# Replace with your PDF path\n",
        "pdf_path = '/content/cse.pdf'  # Update this path\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "print(f\"Extracted text length: {len(pdf_text)} characters\")\n",
        "\n",
        "# Step 2: Preprocess and chunk text\n",
        "def chunk_text(text, max_length=384):  # Reduced max_length for RoBERTa\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) < max_length:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "text_chunks = chunk_text(pdf_text)\n",
        "print(f\"Number of chunks: {len(text_chunks)}\")\n",
        "\n",
        "# Step 3: Generate synthetic question-answer pairs\n",
        "def generate_synthetic_qa(chunks, num_questions=200):  # Increased number of questions\n",
        "    model = SentenceTransformer('all-mpnet-base-v2')  # More powerful embedding model\n",
        "    qa_pairs = []\n",
        "    for chunk in tqdm(chunks, desc=\"Generating QA pairs\"):\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', chunk)\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.strip()) > 20:\n",
        "                # Create multiple question types\n",
        "                question_types = [\n",
        "                    f\"What is mentioned about {sentence[:30].strip()}...?\",\n",
        "                    f\"Can you explain {sentence[:30].strip()}...?\",\n",
        "                    f\"What details are provided about {sentence[:30].strip()}...?\"\n",
        "                ]\n",
        "                for question in question_types:\n",
        "                    qa_pairs.append({\"question\": question, \"answer\": sentence.strip(), \"context\": chunk})\n",
        "                    if len(qa_pairs) >= num_questions:\n",
        "                        break\n",
        "        if len(qa_pairs) >= num_questions:\n",
        "            break\n",
        "    return qa_pairs\n",
        "\n",
        "qa_pairs = generate_synthetic_qa(text_chunks)\n",
        "print(f\"Generated {len(qa_pairs)} QA pairs\")\n",
        "\n",
        "# Step 4: Prepare dataset for fine-tuning\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large')\n",
        "\n",
        "class QADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, qa_pairs):\n",
        "        self.qa_pairs = qa_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        qa = self.qa_pairs[idx]\n",
        "        question = qa['question']\n",
        "        context = qa['context']\n",
        "        answer = qa['answer']\n",
        "\n",
        "        # Encode question and context\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            question,\n",
        "            context,\n",
        "            add_special_tokens=True,\n",
        "            max_length=384,  # RoBERTa optimal length\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Find start and end positions of answer in context\n",
        "        answer_encoding = self.tokenizer.encode(answer, add_special_tokens=False)\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        answer_ids = self.tokenizer.encode(answer, add_special_tokens=False)\n",
        "\n",
        "        start_positions = -1\n",
        "        end_positions = -1\n",
        "        for i in range(len(input_ids) - len(answer_ids)):\n",
        "            if input_ids[i:i+len(answer_ids)].tolist() == answer_ids:\n",
        "                start_positions = i\n",
        "                end_positions = i + len(answer_ids) - 1\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'token_type_ids': encoding['token_type_ids'].squeeze(),\n",
        "            'start_positions': start_positions,\n",
        "            'end_positions': end_positions\n",
        "        }\n",
        "\n",
        "dataset = QADataset(qa_pairs)\n",
        "\n",
        "# Step 5: Fine-tune the model\n",
        "model = RobertaForQuestionAnswering.from_pretrained('roberta-large')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/LLM/2_roberta-large/qa_model_roberta',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,  # Smaller batch size for large model\n",
        "    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch size\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=1e-5,  # Lower learning rate for large model\n",
        "    fp16=True,  # Enable mixed precision for GPU\n",
        "    logging_dir='/content/drive/MyDrive/logs',\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\"  # Disable W&B logging\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Step 6: Save the model\n",
        "model.save_pretrained('/content/drive/MyDrive/LLM/2_roberta-large/qa_model_roberta')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/LLM/2_roberta-large/qa_model_roberta')\n",
        "print(\"Model and tokenizer saved to /content/drive/MyDrive/LLM/2_roberta-large/qa_model_roberta\")\n",
        "\n",
        "# Step 7: Function to answer questions using the saved model with context selection\n",
        "def answer_question(question, chunks, model_path='/content/drive/MyDrive/LLM/2_roberta-large/qa_model_roberta'):\n",
        "    # Load model and tokenizer\n",
        "    model = RobertaForQuestionAnswering.from_pretrained(model_path)\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(model_path)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Use sentence-transformer to find the most relevant chunk\n",
        "    embedder = SentenceTransformer('all-mpnet-base-v2')\n",
        "    question_embedding = embedder.encode(question, convert_to_tensor=True)\n",
        "    chunk_embeddings = embedder.encode(chunks, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(question_embedding, chunk_embeddings)[0]\n",
        "    best_chunk_idx = torch.argmax(cos_scores).item()\n",
        "    context = chunks[best_chunk_idx]\n",
        "\n",
        "    # Encode inputs\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        question,\n",
        "        context,\n",
        "        add_special_tokens=True,\n",
        "        max_length=384,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    start_idx = torch.argmax(start_scores)\n",
        "    end_idx = torch.argmax(end_scores) + 1\n",
        "\n",
        "    answer_tokens = input_ids[0][start_idx:end_idx]\n",
        "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "    return answer, context\n",
        "\n",
        "# Example usage\n",
        "question = \"What is the main topic of the document?\"\n",
        "answer, selected_context = answer_question(question, text_chunks)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")\n",
        "print(f\"Selected context: {selected_context[:100]}...\")  # Print first 100 chars of context"
      ],
      "metadata": {
        "id": "c2QZ2f0ESCzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3_DeepSeek-R1"
      ],
      "metadata": {
        "id": "tpd0EVsOVih0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install pdfplumber transformers torch sentence-transformers datasets accelerate peft huggingface_hub\n",
        "\n",
        "# Import libraries\n",
        "from google.colab import drive\n",
        "import pdfplumber\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 1: Authenticate with Hugging Face\n",
        "# Replace with your Hugging Face Read token from https://huggingface.co/settings/tokens\n",
        "hf_token = \"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\"  # Update with your Read token\n",
        "if hf_token:\n",
        "    login(hf_token)\n",
        "    print(\"Hugging Face login successful\")\n",
        "else:\n",
        "    raise ValueError(\"Hugging Face Read token required for LLaMA 3.1. Set `hf_token` or run `huggingface-cli login`.\")\n",
        "\n",
        "# Step 2: Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text: {e}\")\n",
        "    return text\n",
        "\n",
        "# Replace with your PDF path\n",
        "pdf_path = '/content/drive/MyDrive/cse.pdf'  # Update this path\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "if not pdf_text:\n",
        "    raise ValueError(\"No text extracted from PDF. Check the file path or content.\")\n",
        "print(f\"Extracted text length: {len(pdf_text)} characters\")\n",
        "\n",
        "# Step 3: Preprocess and chunk text\n",
        "def chunk_text(text, max_length=512):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) < max_length:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "text_chunks = chunk_text(pdf_text)\n",
        "print(f\"Number of chunks: {len(text_chunks)}\")\n",
        "\n",
        "# Step 4: Generate synthetic question-answer pairs\n",
        "def generate_synthetic_qa(chunks, num_questions=500):\n",
        "    model = SentenceTransformer('all-mpnet-base-v2')\n",
        "    qa_pairs = []\n",
        "    for chunk in tqdm(chunks, desc=\"Generating QA pairs\"):\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', chunk)\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.strip()) > 20:\n",
        "                question_types = [\n",
        "                    f\"What is discussed about {sentence[:30].strip()}...?\",\n",
        "                    f\"Can you explain {sentence[:30].strip()}...?\",\n",
        "                    f\"What details are given about {sentence[:30].strip()}...?\",\n",
        "                    f\"What is the significance of {sentence[:30].strip()}...?\"\n",
        "                ]\n",
        "                for question in question_types:\n",
        "                    qa_pairs.append({\"question\": question, \"answer\": sentence.strip(), \"context\": chunk})\n",
        "                    if len(qa_pairs) >= num_questions:\n",
        "                        break\n",
        "        if len(qa_pairs) >= num_questions:\n",
        "            break\n",
        "    return qa_pairs\n",
        "\n",
        "qa_pairs = generate_synthetic_qa(text_chunks)\n",
        "print(f\"Generated {len(qa_pairs)} QA pairs\")\n",
        "\n",
        "# Step 5: Prepare dataset for fine-tuning\n",
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B-Instruct')\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, qa_pairs):\n",
        "        self.qa_pairs = qa_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        qa = self.qa_pairs[idx]\n",
        "        question = qa['question']\n",
        "        context = qa['context']\n",
        "        answer = qa['answer']\n",
        "\n",
        "        # Format input for LLaMA 3.1\n",
        "        prompt = f\"<|begin_of_text|>Question: {question}\\nContext: {context}\\nAnswer: {answer}<|end_of_text|>\"\n",
        "        encoding = self.tokenizer(\n",
        "            prompt,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': encoding['input_ids'].squeeze()\n",
        "        }\n",
        "\n",
        "dataset = Dataset.from_list([QADataset(qa_pairs).__getitem__(i) for i in range(len(qa_pairs))])\n",
        "\n",
        "# Step 6: Fine-tune the model with LoRA\n",
        "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct')\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/qa_model_llama3_1',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    logging_dir='/content/drive/MyDrive/logs',\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Step 7: Save the model\n",
        "model.save_pretrained('/content/drive/MyDrive/qa_model_llama3_1')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/qa_model_llama3_1')\n",
        "print(\"Model and tokenizer saved to /content/drive/MyDrive/qa_model_llama3_1\")\n",
        "\n",
        "# Step 8: Function to answer questions using the saved model with context selection\n",
        "def answer_question(question, chunks, model_path='/content/drive/MyDrive/qa_model_llama3_1'):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Select most relevant chunk\n",
        "    embedder = SentenceTransformer('all-mpnet-base-v2')\n",
        "    question_embedding = embedder.encode(question, convert_to_tensor=True)\n",
        "    chunk_embeddings = embedder.encode(chunks, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(question_embedding, chunk_embeddings)[0]\n",
        "    best_chunk_idx = torch.argmax(cos_scores).item()\n",
        "    context = chunks[best_chunk_idx]\n",
        "\n",
        "    # Format input for LLaMA 3.1\n",
        "    prompt = f\"<|begin_of_text|>Question: {question}\\nContext: {context}\\nAnswer: \"\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = answer.split(\"Answer: \")[-1].strip()\n",
        "    return answer, context\n",
        "\n",
        "# Example usage\n",
        "question = \"What is the main topic of the document?\"\n",
        "answer, selected_context = answer_question(question, text_chunks)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")\n",
        "print(f\"Selected context: {selected_context[:100]}...\")"
      ],
      "metadata": {
        "id": "hwrKF-IgSCuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4_Qwen-7B"
      ],
      "metadata": {
        "id": "wZLRFhmabT3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install pdfplumber transformers torch sentence-transformers datasets accelerate peft\n",
        "\n",
        "\n",
        "# Set environment variable to reduce memory fragmentation\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Install dependencies\n",
        "!pip install pdfplumber transformers torch sentence-transformers datasets accelerate peft bitsandbytes\n",
        "\n",
        "# Import libraries\n",
        "from google.colab import drive\n",
        "import pdfplumber\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 1: Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text: {e}\")\n",
        "    return text\n",
        "\n",
        "# Replace with your PDF path\n",
        "pdf_path = '/content/drive/MyDrive/cse.pdf'  # Update this path\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "if not pdf_text:\n",
        "    raise ValueError(\"No text extracted from PDF. Check the file path or content.\")\n",
        "print(f\"Extracted text length: {len(pdf_text)} characters\")\n",
        "\n",
        "# Step 2: Preprocess and chunk text\n",
        "def chunk_text(text, max_length=512):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) < max_length:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "text_chunks = chunk_text(pdf_text)\n",
        "print(f\"Number of chunks: {len(text_chunks)}\")\n",
        "\n",
        "# Step 3: Generate synthetic question-answer pairs\n",
        "def generate_synthetic_qa(chunks, num_questions=500):\n",
        "    model = SentenceTransformer('all-mpnet-base-v2')\n",
        "    qa_pairs = []\n",
        "    for chunk in tqdm(chunks, desc=\"Generating QA pairs\"):\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', chunk)\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.strip()) > 20:\n",
        "                question_types = [\n",
        "                    f\"What is discussed about {sentence[:30].strip()}...?\",\n",
        "                    f\"Can you explain {sentence[:30].strip()}...?\",\n",
        "                    f\"What details are given about {sentence[:30].strip()}...?\",\n",
        "                    f\"What is the significance of {sentence[:30].strip()}...?\"\n",
        "                ]\n",
        "                for question in question_types:\n",
        "                    qa_pairs.append({\"question\": question, \"answer\": sentence.strip(), \"context\": chunk})\n",
        "                    if len(qa_pairs) >= num_questions:\n",
        "                        break\n",
        "        if len(qa_pairs) >= num_questions:\n",
        "            break\n",
        "    return qa_pairs\n",
        "\n",
        "qa_pairs = generate_synthetic_qa(text_chunks)\n",
        "print(f\"Generated {len(qa_pairs)} QA pairs\")\n",
        "\n",
        "# Step 4: Prepare dataset for fine-tuning\n",
        "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2-7B-Instruct')\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, qa_pairs):\n",
        "        self.qa_pairs = qa_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        qa = self.qa_pairs[idx]\n",
        "        question = qa['question']\n",
        "        context = qa['context']\n",
        "        answer = qa['answer']\n",
        "\n",
        "        prompt = f\"Question: {question}\\nContext: {context}\\nAnswer: {answer}\"\n",
        "        encoding = self.tokenizer(\n",
        "            prompt,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': encoding['input_ids'].squeeze()\n",
        "        }\n",
        "\n",
        "dataset = Dataset.from_list([QADataset(qa_pairs).__getitem__(i) for i in range(len(qa_pairs))])\n",
        "\n",
        "# Step 5: Fine-tune the model with LoRA\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'Qwen/Qwen2-7B-Instruct',\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.gradient_checkpointing_enable()\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/qa_model_qwen2',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=16,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    logging_dir='/content/drive/MyDrive/logs',\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",\n",
        "    optim=\"adamw_8bit\"  # Requires bitsandbytes\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Step 6: Save the model\n",
        "model.save_pretrained('/content/drive/MyDrive/qa_model_qwen2')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/qa_model_qwen2')\n",
        "print(\"Model and tokenizer saved to /content/drive/MyDrive/qa_model_qwen2\")\n",
        "\n",
        "# Step 7: Function to answer questions using the saved model with context selection\n",
        "def answer_question(question, chunks, model_path='/content/drive/MyDrive/qa_model_qwen2'):\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Select most relevant chunk\n",
        "    embedder = SentenceTransformer('all-mpnet-base-v2')\n",
        "    question_embedding = embedder.encode(question, convert_to_tensor=True)\n",
        "    chunk_embeddings = embedder.encode(chunks, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(question_embedding, chunk_embeddings)[0]\n",
        "    best_chunk_idx = torch.argmax(cos_scores).item()\n",
        "    context = chunks[best_chunk_idx]\n",
        "\n",
        "    prompt = f\"Question: {question}\\nContext: {context}\\nAnswer: \"\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = answer.split(\"Answer: \")[-1].strip()\n",
        "    return answer, context\n",
        "\n",
        "# Example usage\n",
        "question = \"Phone number of the teachers?\"\n",
        "answer, selected_context = answer_question(question, text_chunks)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")\n",
        "print(f\"Selected context: {selected_context[:100]}...\")"
      ],
      "metadata": {
        "id": "A1TbdSOpSCoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install pdfplumber transformers torch sentence-transformers datasets\n",
        "\n",
        "# Import libraries\n",
        "from google.colab import drive\n",
        "import pdfplumber\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 1: Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text: {e}\")\n",
        "    return text\n",
        "\n",
        "# Replace with your PDF path\n",
        "pdf_path = '/content/drive/MyDrive/cse.pdf'  # Update this path\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "if not pdf_text:\n",
        "    raise ValueError(\"No text extracted from PDF. Check the file path or content.\")\n",
        "print(f\"Extracted text length: {len(pdf_text)} characters\")\n",
        "\n",
        "# Step 2: Preprocess and chunk text\n",
        "def chunk_text(text, max_length=512):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) < max_length:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "text_chunks = chunk_text(pdf_text)\n",
        "print(f\"Number of chunks: {len(text_chunks)}\")\n",
        "\n",
        "## Step 3: Function to answer questions using the saved model with context selection\n",
        "# def answer_question(question, chunks, model_path='/content/drive/MyDrive/LLM/4_Qwen/qa_model_qwen2'):\n",
        "#     model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#     model.to(device)\n",
        "#     model.eval()\n",
        "\n",
        "#     # Select most relevant chunk\n",
        "#     embedder = SentenceTransformer('all-mpnet-base-v2')\n",
        "#     question_embedding = embedder.encode(question, convert_to_tensor=True)\n",
        "#     chunk_embeddings = embedder.encode(chunks, convert_to_tensor=True)\n",
        "#     cos_scores = util.cos_sim(question_embedding, chunk_embeddings)[0]\n",
        "#     best_chunk_idx = torch.argmax(cos_scores).item()\n",
        "#     context = chunks[best_chunk_idx]\n",
        "\n",
        "#     # Format input for Qwen2\n",
        "#     prompt = f\"Question: {question}\\nContext: {context}\\nAnswer: \"\n",
        "#     inputs = tokenizer(\n",
        "#         prompt,\n",
        "#         add_special_tokens=True,\n",
        "#         max_length=512,\n",
        "#         return_tensors='pt',\n",
        "#         truncation=True\n",
        "#     )\n",
        "\n",
        "#     input_ids = inputs['input_ids'].to(device)\n",
        "#     attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.generate(\n",
        "#             input_ids=input_ids,\n",
        "#             attention_mask=attention_mask,\n",
        "#             max_new_tokens=100,\n",
        "#             do_sample=False\n",
        "#         )\n",
        "\n",
        "#     answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "#     answer = answer.split(\"Answer: \")[-1].strip()\n",
        "#     return answer, context\n",
        "\n",
        "\n",
        "from transformers import TextIteratorStreamer\n",
        "import threading\n",
        "\n",
        "def answer_question(question, chunks, model_path='/content/drive/MyDrive/LLM/4_Qwen/qa_model_qwen2'):\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map='auto'\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "\n",
        "    # Find the most relevant chunk\n",
        "    embedder = SentenceTransformer('all-mpnet-base-v2')\n",
        "    question_embedding = embedder.encode(question, convert_to_tensor=True)\n",
        "    chunk_embeddings = embedder.encode(chunks, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(question_embedding, chunk_embeddings)[0]\n",
        "    best_chunk_idx = torch.argmax(cos_scores).item()\n",
        "    context = chunks[best_chunk_idx]\n",
        "\n",
        "    # Format input\n",
        "    prompt = f\"Question: {question}\\nContext: {context}\\nAnswer: \"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
        "\n",
        "    # Use a TextIteratorStreamer for real-time output\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "    generation_kwargs = dict(\n",
        "        **inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.8,\n",
        "    )\n",
        "\n",
        "    # Generate in background thread to stream output\n",
        "    thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    print(\"Answer:\")\n",
        "    full_answer = \"\"\n",
        "    for token in streamer:\n",
        "        print(token, end='', flush=True)\n",
        "        full_answer += token\n",
        "\n",
        "    print(\"\\n\")\n",
        "    return full_answer.strip(), context\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "question = \"What is the main topic of the document?\"\n",
        "answer, selected_context = answer_question(question, text_chunks)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")\n",
        "print(f\"Selected context: {selected_context[:100]}...\")\n"
      ],
      "metadata": {
        "id": "I_ssje1ROLw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"Who are the teachers?\"\n",
        "answer, selected_context = answer_question(question, text_chunks)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")\n",
        "print(f\"Selected context: {selected_context[:100]}...\")"
      ],
      "metadata": {
        "id": "3WSewuhTRxMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5_Retrieval-Augmented Generation (RAG) or document QA with a powerful model like LLaMA 3, Mistral, or GPT-style"
      ],
      "metadata": {
        "id": "DnMawLkdTUNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean old broken installs\n",
        "!pip uninstall -y faiss-cpu faiss-gpu faiss farm-haystack\n",
        "\n",
        "# Install compatible FAISS and Haystack\n",
        "!pip install -q faiss-cpu==1.7.4\n",
        "!pip install -q farm-haystack==1.17.1\n",
        "!pip install -q pypdf sentence-transformers\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]  # Automatically gets the uploaded filename\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from haystack.document_stores import FAISSDocumentStore\n",
        "from haystack.nodes import EmbeddingRetriever, TransformersReader, PreProcessor\n",
        "from haystack.pipelines import ExtractiveQAPipeline\n",
        "from haystack import Document\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Extract text from PDF\n",
        "def extract_pdf_text(path):\n",
        "    reader = PdfReader(path)\n",
        "    return \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
        "\n",
        "text = extract_pdf_text(pdf_path)\n",
        "\n",
        "# Split into smaller chunks\n",
        "preprocessor = PreProcessor(split_by=\"word\", split_length=200, split_respect_sentence_boundary=True)\n",
        "docs = preprocessor.process([Document(content=text)])\n",
        "\n",
        "# Set up FAISS vector store\n",
        "document_store = FAISSDocumentStore(embedding_dim=768)\n",
        "\n",
        "# Use sentence-transformer for embeddings\n",
        "retriever = EmbeddingRetriever(\n",
        "    document_store=document_store,\n",
        "    embedding_model=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "    use_gpu=True\n",
        ")\n",
        "\n",
        "document_store.write_documents(docs)\n",
        "document_store.update_embeddings(retriever)\n",
        "\n",
        "# Use a strong reader model\n",
        "reader = TransformersReader(\n",
        "    model_name_or_path=\"google/flan-t5-base\",  # You can also try flan-t5-xl if enough GPU\n",
        "    tokenizer=\"google/flan-t5-base\",\n",
        "    use_gpu=True\n",
        ")\n",
        "\n",
        "# Create pipeline\n",
        "pipe = ExtractiveQAPipeline(reader=reader, retriever=retriever)\n",
        "\n",
        "# Ask your question\n",
        "query = \"What is the phone number of John Doe?\"  # Change this!\n",
        "prediction = pipe.run(query=query, params={\"Retriever\": {\"top_k\": 5}, \"Reader\": {\"top_k\": 1}})\n",
        "\n",
        "# Show answer\n",
        "print(\"Answer:\", prediction[\"answers\"][0].answer)\n"
      ],
      "metadata": {
        "id": "dacv8rRnTTfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6_Open Ai with langchain"
      ],
      "metadata": {
        "id": "0dNSlUb6XAdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Uninstall everything causing conflict\n",
        "!pip uninstall -y farm-haystack protobuf pydantic typing-extensions langchain-core langchain numpy torch torchaudio\n",
        "\n",
        "# STEP 2: Reinstall correct compatible versions\n",
        "!pip install -q \\\n",
        "  \"protobuf>=3.20.3,<6.0.0\" \\\n",
        "  \"pydantic>=2.7.4\" \\\n",
        "  \"typing-extensions>=4.11.0\" \\\n",
        "  \"numpy<2\" \\\n",
        "  langchain \\\n",
        "  openai \\\n",
        "  faiss-cpu \\\n",
        "  pymupdf \\\n",
        "  tiktoken\n",
        "\n",
        "# STEP 3: ✅ Test imports\n",
        "try:\n",
        "    from langchain.text_splitter import CharacterTextSplitter\n",
        "    print(\"✅ LangChain is working.\")\n",
        "except ImportError as e:\n",
        "    print(\"❌ Import failed:\", e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ✅ STEP 1: Install Clean Packages\n",
        "!pip install -q langchain openai faiss-cpu pymupdf tiktoken\n",
        "\n",
        "# ✅ STEP 2: Set your OpenAI API key\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ✅ STEP 1: Uninstall conflicting packages\n",
        "!pip uninstall -y farm-haystack protobuf pydantic typing-extensions torch torchaudio\n",
        "\n",
        "# ✅ STEP 2: Install only what you need for LangChain PDF QA\n",
        "!pip install -q langchain openai faiss-cpu pymupdf tiktoken pydantic typing-extensions\n",
        "\n",
        "# # ✅ STEP 3: Set up OpenAI API Key\n",
        "# import os\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Replace with your real key\n",
        "\n",
        "# ✅ STEP 4: Load PDF from path\n",
        "import fitz  # PyMuPDF\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "def load_pdf(path):\n",
        "    doc = fitz.open(path)\n",
        "    texts = [page.get_text() for page in doc]\n",
        "    full_text = \"\\n\".join(texts)\n",
        "    return full_text\n",
        "\n",
        "pdf_path = '/content/drive/MyDrive/cse.pdf'\n",
        "text = load_pdf(pdf_path)\n",
        "\n",
        "# ✅ STEP 5: Split Text into Chunks\n",
        "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "chunks = splitter.create_documents([text])\n",
        "\n",
        "# ✅ STEP 6: Embed with OpenAI and store in FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "db = FAISS.from_documents(chunks, embedding)\n",
        "\n",
        "# ✅ STEP 7: Ask Questions via RetrievalQA\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=ChatOpenAI(temperature=0),\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever()\n",
        ")\n",
        "\n",
        "# ✅ STEP 8: Ask a question\n",
        "query = \"What is the phone number of John Doe?\"\n",
        "result = qa.run(query)\n",
        "print(\"🔍 Answer:\", result)\n",
        "\n"
      ],
      "metadata": {
        "id": "fhqFJ98dXAPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7_all-MiniLM-L6-v2"
      ],
      "metadata": {
        "id": "vYm9G3Rxb9x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install all dependencies (run this first if in Colab)\n",
        "!pip install -q faiss-cpu sentence-transformers transformers pymupdf\n",
        "\n",
        "# STEP 2: Import all necessary modules\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "# STEP 3: Load and extract text from PDF\n",
        "def load_pdf(path):\n",
        "    doc = fitz.open(path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# STEP 4: Split the text into chunks\n",
        "def split_text(text, max_len=500):\n",
        "    sentences = text.split(\". \")\n",
        "    chunks, chunk = [], \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(chunk) + len(sentence) <= max_len:\n",
        "            chunk += sentence + \". \"\n",
        "        else:\n",
        "            chunks.append(chunk.strip())\n",
        "            chunk = sentence + \". \"\n",
        "    chunks.append(chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "# STEP 5: Embed chunks using sentence transformer\n",
        "def embed_chunks(chunks):\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    embeddings = model.encode(chunks)\n",
        "    return model, embeddings\n",
        "\n",
        "# STEP 6: Create FAISS index\n",
        "def create_faiss_index(embeddings):\n",
        "    dim = embeddings[0].shape[0]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(np.array(embeddings))\n",
        "    return index\n",
        "\n",
        "# STEP 7: Load QA model and answer questions\n",
        "def answer_question(query, index, chunks, embedder, top_k=5):\n",
        "    query_embedding = embedder.encode([query])\n",
        "    scores, indices = index.search(np.array(query_embedding), top_k)\n",
        "    top_chunks = [chunks[i] for i in indices[0]]\n",
        "\n",
        "    qa = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "    answers = []\n",
        "    for context in top_chunks:\n",
        "        result = qa(question=query, context=context)\n",
        "        answers.append((result[\"score\"], result[\"answer\"]))\n",
        "\n",
        "    best = sorted(answers, key=lambda x: x[0], reverse=True)[0]\n",
        "    return best[1]\n",
        "\n",
        "# STEP 8: Run everything\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_path = \"/content/drive/MyDrive/cse.pdf\"  # UPDATE to your actual path\n",
        "    raw_text = load_pdf(pdf_path)\n",
        "    chunks = split_text(raw_text)\n",
        "    embedder, embeddings = embed_chunks(chunks)\n",
        "    index = create_faiss_index(embeddings)\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nAsk a question (or type 'exit'): \")\n",
        "        if query.lower() == \"exit\":\n",
        "            break\n",
        "        answer = answer_question(query, index, chunks, embedder)\n",
        "        print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "id": "OYCSChkChNdc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}